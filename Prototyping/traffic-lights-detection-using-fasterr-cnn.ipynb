{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-29T17:18:16.648755Z","iopub.status.busy":"2024-05-29T17:18:16.648432Z","iopub.status.idle":"2024-05-29T17:18:20.575565Z","shell.execute_reply":"2024-05-29T17:18:20.574513Z","shell.execute_reply.started":"2024-05-29T17:18:16.648722Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","\n","import cv2\n","import time\n","import pandas as pd\n","import numpy as np\n","import json\n","\n","from PIL import Image\n","import torch \n","import torchvision\n","import torchvision.transforms as T\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torch.utils.data import DataLoader, Dataset\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","DIR_INPUT = '/kaggle/input/traffic-light-detection-dataset/'\n","DIR_IMAGES = DIR_INPUT + \"train_dataset/train_images/\"\n","DIR_IMAGES_TEST = DIR_INPUT + \"test_dataset/test_images/\""]},{"cell_type":"markdown","metadata":{},"source":["## Extract labels about bounding box and light color from json file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:20.578274Z","iopub.status.busy":"2024-05-29T17:18:20.577654Z","iopub.status.idle":"2024-05-29T17:18:21.100127Z","shell.execute_reply":"2024-05-29T17:18:21.099014Z","shell.execute_reply.started":"2024-05-29T17:18:20.578230Z"},"trusted":true},"outputs":[],"source":["with open(\"/kaggle/input/traffic-light-detection-dataset/train_dataset/train.json\") as f:\n","    data_dict = json.load(f)\n","\n","# Create a list containing the data rows of the table\n","data = []\n","\n","# Loop through the elements in the annotations list\n","for annotation in data_dict['annotations']:\n","    # Get general bounding box information\n","    filename = annotation['filename']\n","    xmin = annotation['bndbox']['xmin']\n","    ymin = annotation['bndbox']['ymin']\n","    xmax = annotation['bndbox']['xmax']\n","    ymax = annotation['bndbox']['ymax']\n","    \n","    if annotation['inbox']:\n","        for inbox in annotation['inbox']:\n","            color = inbox['color']\n","            data.append({\n","                    'filename': filename,\n","                    'xmin': xmin,\n","                    'ymin': ymin,\n","                    'xmax': xmax,\n","                    'ymax': ymax,\n","                    'color': color,\n","                })\n","    \n","\n","# Create a DataFrame from a list of data and save it to a CSV file\n","df = pd.DataFrame(data)\n","df.to_csv('traffic_lights.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.102048Z","iopub.status.busy":"2024-05-29T17:18:21.101624Z","iopub.status.idle":"2024-05-29T17:18:21.119207Z","shell.execute_reply":"2024-05-29T17:18:21.118174Z","shell.execute_reply.started":"2024-05-29T17:18:21.102002Z"},"trusted":true},"outputs":[],"source":["data_tf = pd.read_csv('/kaggle/working/traffic_lights.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.122694Z","iopub.status.busy":"2024-05-29T17:18:21.122279Z","iopub.status.idle":"2024-05-29T17:18:21.194391Z","shell.execute_reply":"2024-05-29T17:18:21.193503Z","shell.execute_reply.started":"2024-05-29T17:18:21.122652Z"},"trusted":true},"outputs":[],"source":["data_tf['filename'] = data_tf['filename'].str.replace('train_images\\\\\\\\', '', regex=True)\n","data_tf.to_csv('traffic_lights_labels.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.196135Z","iopub.status.busy":"2024-05-29T17:18:21.195718Z","iopub.status.idle":"2024-05-29T17:18:21.231896Z","shell.execute_reply":"2024-05-29T17:18:21.230792Z","shell.execute_reply.started":"2024-05-29T17:18:21.196104Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/working/traffic_lights_labels.csv')\n","df"]},{"cell_type":"markdown","metadata":{},"source":["## Data exploration and visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.233954Z","iopub.status.busy":"2024-05-29T17:18:21.233594Z","iopub.status.idle":"2024-05-29T17:18:21.248058Z","shell.execute_reply":"2024-05-29T17:18:21.247060Z","shell.execute_reply.started":"2024-05-29T17:18:21.233923Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.249512Z","iopub.status.busy":"2024-05-29T17:18:21.249191Z","iopub.status.idle":"2024-05-29T17:18:21.264443Z","shell.execute_reply":"2024-05-29T17:18:21.263376Z","shell.execute_reply.started":"2024-05-29T17:18:21.249481Z"},"trusted":true},"outputs":[],"source":["unq_values = df[\"filename\"].unique()\n","print(\"Total Records: \", len(df))\n","print(\"Unique Images: \",len(unq_values))\n","\n","null_values = df.isnull().sum(axis = 0)\n","print(\"\\n> Null Values in each column <\")\n","print(null_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.266021Z","iopub.status.busy":"2024-05-29T17:18:21.265615Z","iopub.status.idle":"2024-05-29T17:18:21.272993Z","shell.execute_reply":"2024-05-29T17:18:21.271964Z","shell.execute_reply.started":"2024-05-29T17:18:21.265977Z"},"trusted":true},"outputs":[],"source":["classes = df[\"color\"].unique()\n","print(\"Total Classes: \",len(classes))\n","print(\"\\n> Classes <\\n\",classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.274658Z","iopub.status.busy":"2024-05-29T17:18:21.274259Z","iopub.status.idle":"2024-05-29T17:18:21.520611Z","shell.execute_reply":"2024-05-29T17:18:21.519519Z","shell.execute_reply.started":"2024-05-29T17:18:21.274616Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(6, 4))\n","plt.title('Class Distribution', fontsize= 14)\n","sns.countplot(x = \"color\", data = df);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.525042Z","iopub.status.busy":"2024-05-29T17:18:21.524693Z","iopub.status.idle":"2024-05-29T17:18:21.537064Z","shell.execute_reply":"2024-05-29T17:18:21.535972Z","shell.execute_reply.started":"2024-05-29T17:18:21.525012Z"},"trusted":true},"outputs":[],"source":["def plot_img(image_name):\n","    \n","    fig, ax = plt.subplots(2, 1, figsize = (14, 14))\n","    ax = ax.flatten()\n","    \n","    bbox = df[df['filename'] == image_name]\n","    img_path = os.path.join(DIR_IMAGES, image_name)\n","    \n","    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","    image2 = image\n","    \n","    ax[0].set_title('Original Image')\n","    ax[0].imshow(image)\n","    \n","    for idx, row in bbox.iterrows():\n","        x1 = row['xmin']\n","        y1 = row['ymin']\n","        x2 = row['xmax']\n","        y2 = row['ymax']\n","        label = row['color']\n","        if label == 'red':\n","            color_brg = (255,0,0)\n","        elif label == 'green':\n","            color_brg = (0,255,0)\n","        elif label == 'yellow':\n","            color_brg = (0,255,255)\n","        \n","        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), color_brg, 3)\n","        font = cv2.FONT_HERSHEY_SIMPLEX\n","        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, color_brg, 2)\n","    \n","    ax[1].set_title('Image with Bondary Box')\n","    ax[1].imshow(image2)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:21.538692Z","iopub.status.busy":"2024-05-29T17:18:21.538352Z","iopub.status.idle":"2024-05-29T17:18:23.913333Z","shell.execute_reply":"2024-05-29T17:18:23.912138Z","shell.execute_reply.started":"2024-05-29T17:18:21.538627Z"},"trusted":true},"outputs":[],"source":["plot_img(\"00004.jpg\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Dataset for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:23.914914Z","iopub.status.busy":"2024-05-29T17:18:23.914578Z","iopub.status.idle":"2024-05-29T17:18:23.921693Z","shell.execute_reply":"2024-05-29T17:18:23.920707Z","shell.execute_reply.started":"2024-05-29T17:18:23.914878Z"},"trusted":true},"outputs":[],"source":["_classes = np.insert(classes, 0, \"background\", axis=0)        \n","class_to_int = {_classes[i] : i for i in range(len(_classes))}\n","int_to_class = {i : _classes[i] for i in range(len(_classes))}\n","print(\"class_to_int : \\n\",class_to_int)\n","print(\"\\nint_to_class : \\n\",int_to_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:23.923537Z","iopub.status.busy":"2024-05-29T17:18:23.923218Z","iopub.status.idle":"2024-05-29T17:18:23.941014Z","shell.execute_reply":"2024-05-29T17:18:23.939935Z","shell.execute_reply.started":"2024-05-29T17:18:23.923507Z"},"trusted":true},"outputs":[],"source":["class TrafficLightDetectionDataset(Dataset):\n","    \n","    def __init__(self, dataframe, image_dir, mode='train', transforms=None, resize_factor=0.25):\n","        \n","        super().__init__()\n","        \n","        self.image_names = dataframe['filename'].unique()\n","        self.df = dataframe\n","        self.image_dir = image_dir\n","        self.transforms = transforms\n","        self.mode = mode\n","        self.resize_factor = resize_factor\n","        \n","    def __len__(self):\n","        return len(self.image_names)\n","        \n","    def __getitem__(self, index: int):\n","        \n","        #Retrive Image name and its records (x1, y1, x2, y2, classname) from df\n","        image_name = self.image_names[index]\n","        records = self.df[self.df['filename'] == image_name]\n","        \n","        #Loading Image\n","        image = cv2.imread(self.image_dir + image_name, cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        \n","        if self.mode == 'train':\n","            \n","            #Get bounding box co-ordinates for each box\n","            boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n","\n","            #Getting labels for each box\n","            temp_labels = records[['color']].values\n","            labels = []\n","            for label in temp_labels:\n","                label = class_to_int[label[0]]\n","                labels.append(label)\n","\n","            #Converting boxes & labels into torch tensor\n","            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","            labels = torch.as_tensor(labels, dtype=torch.int64)\n","            \n","            # Resize the image and the bounding boxes\n","            height, width, _ = image.shape\n","            new_height, new_width = int(height * self.resize_factor), int(width * self.resize_factor)\n","            image = cv2.resize(image, (new_width, new_height))\n","            boxes = boxes * self.resize_factor\n","            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","            \n","            #Creating target\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['area'] = torch.as_tensor(area, dtype=torch.float32)\n","\n","            #Transforms\n","            if self.transforms:\n","                image = self.transforms(image)\n","\n","\n","            return image, target, image_name\n","        \n","        elif self.mode == 'test':\n","            # Resize the image\n","            height, width, _ = image.shape\n","            new_height, new_width = int(height * self.resize_factor), int(width * self.resize_factor)\n","            image = cv2.resize(image, (new_width, new_height))\n","\n","            if self.transforms:\n","                image = self.transforms(image)\n","\n","            return image, image_name\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:23.942482Z","iopub.status.busy":"2024-05-29T17:18:23.942208Z","iopub.status.idle":"2024-05-29T17:18:23.975655Z","shell.execute_reply":"2024-05-29T17:18:23.974615Z","shell.execute_reply.started":"2024-05-29T17:18:23.942455Z"},"trusted":true},"outputs":[],"source":["# Preparing data for Train & Validation\n","\n","def get_transform():\n","    return T.Compose([T.ToTensor()])\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","\n","# Dataset object\n","dataset = TrafficLightDetectionDataset(df, DIR_IMAGES, transforms = get_transform())\n","\n","\n","# split the dataset in train and test set - using 80% for training, 20% for validation\n","indices = torch.randperm(len(dataset)).tolist()\n","train_dataset = torch.utils.data.Subset(dataset, indices[:-490])\n","valid_dataset = torch.utils.data.Subset(dataset, indices[-490:])\n","\n","\n","# Preparing data loaders\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size = 2,\n","    shuffle = True,\n","    num_workers = 2,\n","    collate_fn = collate_fn\n",")\n","\n","\n","valid_data_loader = DataLoader(\n","    valid_dataset,\n","    batch_size = 2,\n","    shuffle = True,\n","    num_workers = 2,\n","    collate_fn = collate_fn\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:23.977101Z","iopub.status.busy":"2024-05-29T17:18:23.976824Z","iopub.status.idle":"2024-05-29T17:18:24.031214Z","shell.execute_reply":"2024-05-29T17:18:24.030055Z","shell.execute_reply.started":"2024-05-29T17:18:23.977074Z"},"trusted":true},"outputs":[],"source":["# Utilize GPU if available\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["## Create model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:24.033083Z","iopub.status.busy":"2024-05-29T17:18:24.032677Z","iopub.status.idle":"2024-05-29T17:18:24.765087Z","shell.execute_reply":"2024-05-29T17:18:24.764209Z","shell.execute_reply.started":"2024-05-29T17:18:24.033044Z"},"trusted":true},"outputs":[],"source":["# Create / load model\n","\n","# Faster - RCNN Model - pretrained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None)\n","num_classes = len(class_to_int)\n","\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:24.766561Z","iopub.status.busy":"2024-05-29T17:18:24.766263Z","iopub.status.idle":"2024-05-29T17:18:27.562921Z","shell.execute_reply":"2024-05-29T17:18:27.562025Z","shell.execute_reply.started":"2024-05-29T17:18:24.766533Z"},"trusted":true},"outputs":[],"source":["# Preparing model for training\n","\n","# Retriving all trainable parameters from model (for optimizer)\n","params = [p for p in model.parameters() if p.requires_grad]\n","# Defininig Optimizer\n","optimizer = torch.optim.Adam(params, lr = 0.0001)\n","# LR\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n","model.to(device)\n","# No of epochs\n","epochs = 5"]},{"cell_type":"markdown","metadata":{},"source":["## Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:18:27.564734Z","iopub.status.busy":"2024-05-29T17:18:27.564428Z"},"trusted":true},"outputs":[],"source":["# Training model\n","\n","itr = 1\n","total_train_loss = []\n","for epoch in range(epochs):\n","    start_time = time.time()\n","    train_loss = []\n","    for images, targets, image_names in tqdm(train_data_loader):\n","        # tqdm \n","        \n","        # Loading images & targets on device\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        \n","        # Forward propagation\n","        out = model(images, targets)\n","        losses = sum(loss for loss in out.values())\n","        \n","        # Reseting Gradients\n","        optimizer.zero_grad()\n","        \n","        # Back propagation\n","        losses.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        \n","        # Average loss\n","        loss_value = losses.item()\n","        train_loss.append(loss_value)\n","        \n","        if itr % 300 == 0:\n","            print(f\"\\n Iteration #{itr} loss: {loss_value:.4f} \\n\") ## {out}\n","        itr += 1\n","    lr_scheduler.step() \n","\n","    epoch_train_loss = np.mean(train_loss)\n","    total_train_loss.append(epoch_train_loss)\n","    print(f'Epoch: {epoch+1}')\n","    print(f'Epoch train loss is {epoch_train_loss:.4f}')\n","\n","    \n","    time_elapsed = time.time() - start_time\n","    print(\"Time elapsed: \",time_elapsed)\n","    \n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': epoch_train_loss\n","            }, \"checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(total_train_loss)\n","plt.title('Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["itr = 1\n","v_loss = []\n","\n","start_time = time.time()\n","\n","for images, targets, image_names in tqdm(valid_data_loader):\n","        \n","    #Loading images & targets on device\n","    images = list(image.to(device) for image in images)\n","    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","        \n","    #Forward propagation\n","    out = model(images, targets)\n","    losses = sum(loss for loss in out.values())\n","        \n","    #Average loss\n","    loss_value = losses.item()\n","    v_loss.append(loss_value)\n","\n","val_loss = np.mean(v_loss)\n","print(f'Val loss is {val_loss:.4f}')\n"," \n","time_elapsed = time.time() - start_time\n","print(\"Time elapsed: \",time_elapsed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["folder_path = \"/kaggle/input/traffic-light-detection-dataset/test_dataset/test_images\"\n","\n","# lấy danh sách các tệp ảnh và tạo một DataFrame trong pandas\n","file_names = os.listdir(folder_path)\n","df_test = pd.DataFrame({'filename': file_names})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(columns = [\"filename\", \"xmin\", \"ymin\", \n","                                     \"xmax\", \"ymax\", \"color\"])\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Preparing Training Data\n","images = os.listdir(DIR_IMAGES_TEST)\n","\n","df_test = df_test.drop_duplicates(subset='filename', keep=\"first\")\n","\n","# Test Dataset\n","test_dataset = TrafficLightDetectionDataset(df_test, DIR_IMAGES_TEST, mode = 'test', \n","                                            transforms = get_transform())\n","\n","# Test data loader\n","test_data_loader = DataLoader(\n","    test_dataset,\n","    batch_size=2,\n","    shuffle=False,\n","    num_workers=2,\n","    drop_last=False,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Results - may take few mins, please wait!\n","\n","threshold = 0.7\n","model.eval()\n","\n","for images, image_names in test_data_loader:\n","\n","    # Forward ->\n","    images = list(image.to(device) for image in images)\n","    output = model(images)\n","    \n","    # Converting tensors to array\n","    boxes = output[0]['boxes'].data.cpu().numpy()\n","    scores = output[0]['scores'].data.cpu().numpy()\n","    labels = output[0]['labels'].data.cpu().numpy()\n","\n","    # Thresholding\n","    boxes_th = boxes[scores >= threshold].astype(np.int32)\n","    scores_th = scores[scores >= threshold]\n","\n","    # int_to_class - labels\n","    labels_th = []\n","    \n","    for x in range(len(labels)):\n","        if scores[x] > threshold:\n","            labels_th.append(int_to_class[labels[x]])\n","    #Appending results to csv\n","    for y in range(len(boxes_th)):\n","        \n","        #Bboxes, classname & image name\n","        x1 = boxes_th[y][0] * 4\n","        y1 = boxes_th[y][1] * 4\n","        x2 = boxes_th[y][2] * 4\n","        y2 = boxes_th[y][3] * 4\n","        class_name = labels_th[y]\n","        \n","        \n","        # Creating row for df\n","        row = {\"filename\" : image_names[0], \n","               \"xmin\" : x1, \"xmax\" : x2, \n","               \"ymin\" : y1, \"ymax\" : y2, \n","               \"color\" : class_name}\n","        \n","        # Appending to df\n","        submission = submission.append(row, ignore_index = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/working/submission.csv')\n","df.head(15)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DIR_INPUT = '/kaggle/input/traffic-light-detection-dataset/'\n","\n","DIR_IMAGES = DIR_INPUT + \"train_dataset/train_images/\"\n","DIR_IMAGES_TEST = DIR_INPUT + \"test_dataset/test_images/\"\n","\n","def plot_img_test(image_name):\n","    \n","    fig, ax = plt.subplots(2, 1, figsize = (14, 14))\n","    ax = ax.flatten()\n","    \n","    bbox = df[df['filename'] == image_name]\n","    img_path = os.path.join(DIR_IMAGES_TEST, image_name)\n","    \n","    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    image /= 255.0\n","    image2 = image\n","    \n","    ax[0].set_title('Original Image')\n","    ax[0].imshow(image)\n","    \n","    for idx, row in bbox.iterrows():\n","        x1 = row['xmin']\n","        y1 = row['ymin']\n","        x2 = row['xmax']\n","        y2 = row['ymax']\n","        label = row['color']\n","        if label == 'red':\n","            color_brg = (255,0,0)\n","        elif label == 'green':\n","            color_brg = (0,255,0)\n","        elif label == 'yellow':\n","            color_brg = (0,255,255)\n","        \n","        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), color_brg, 3)\n","        font = cv2.FONT_HERSHEY_SIMPLEX\n","        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, color_brg, 2)\n","    \n","    ax[1].set_title('Image with Bondary Box')\n","    ax[1].imshow(image2)\n","\n","    plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1805330,"sourceId":2944676,"sourceType":"datasetVersion"}],"dockerImageVersionId":30461,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":4}
